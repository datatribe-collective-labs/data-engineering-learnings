# NYC Yellow Taxi Data Pipeline ğŸš•ğŸ“Š

This project demonstrates a complete data pipeline built for NYC Yellow Taxi trip data.  
It covers real-world ELT steps: downloading raw data, cleaning it with PySpark, uploading to BigQuery, and visualizing trends via Streamlit.

Everything runs locally in Docker, but cloud operations (like storage and querying) are delegated to GCP services like BigQuery.  
The pipeline is also fully orchestrated using Airflow, with container builds automated through GitHub Actions.

---

### ğŸ”§ Tech Stack

- **Docker** â€“ Isolates each service (Airflow, Streamlit, etc.) for portability and reproducibility  
- **PySpark** â€“ Cleans and transforms Parquet datasets at scale  
- **Google BigQuery** â€“ Cloud-based warehouse for analytics-ready storage  
- **Airflow** â€“ Automates the pipeline via DAG scheduling  
- **Streamlit** â€“ Provides a simple dashboard to visualize daily and hourly metrics  
- **Pandas & SQLAlchemy** â€“ Used for lightweight querying and quick data previews  
  *(SQLAlchemy acts as a connector layer, making it easier to write to BigQuery from Python code.)*

### ğŸ—‚ï¸ System Architecture

The pipeline follows a modular design that separates raw data ingestion, cleaning, cloud storage, orchestration, and frontend rendering.

1. **Download Raw Parquet Data**  
   A Python script pulls NYC Yellow Taxi trip records from the TLC S3 repository.  
   The download is customizable via environment variables for year/month selection.

2. **Clean & Process Locally with PySpark**  
   Data is filtered and transformed locally using PySpark, including dropping invalid records, computing trip duration, and extracting hourly summaries.

3. **Upload to BigQuery**  
   The cleaned dataset is written to a BigQuery table using the `google-cloud-bigquery` Python client.

4. **Orchestration with Airflow**  
   A DAG defines the pipeline steps (from download to upload).  
   Each task runs inside containers defined via Docker Compose.

5. **Streamlit Dashboard**  
   A lightweight dashboard visualizes key metrics:  
   - Average fare and tip rate  
   - Trip volume by hour  
   - Distance patterns  
   - Passenger counts

6. **(Optional) GitHub Actions CI/CD**  
   Docker images for Airflow and Streamlit are automatically built and pushed to Docker Hub upon each commit to `main` using GitHub Actions.  
   This makes redeployment easier across environments.

> âš™ï¸ A diagram illustrating this system will be added later to this section.

### ğŸ§ª Local Setup

This project was developed and tested on **Windows 11 using WSL2 (Ubuntu)**, which provides a Linux-compatible environment inside Windows.  
If you're using **macOS** or **Linux**, you can skip WSL and run everything directly in your terminal.

All componentsâ€”Airflow, PySpark, and Streamlitâ€”are containerized using Docker Compose, so you donâ€™t need to install them manually.

---

#### âœ… Prerequisites

Make sure the following are installed:

- [Docker Desktop](https://www.docker.com/products/docker-desktop/)  
- [Python 3.10+](https://www.python.org/) (only if running any standalone scripts)
- [Google Cloud SDK](https://cloud.google.com/sdk) (`gcloud` CLI)
- A Google Cloud project with BigQuery enabled
- A BigQuery dataset (create manually or let the script handle it)

---

#### ğŸ› ï¸ Setup Steps

1. **Clone the repository**

   ```bash
   git clone https://github.com/your-username/nyc-taxi-pipeline.git
   cd nyc-taxi-pipeline
   ```

2. **Copy and configure environment files**

   Create your own `.env` file:
   ```bash
   cp .env.example .env
   ```

   Then set your local user ID (optional, improves file permissions in WSL/Linux):
   ```bash
   export AIRFLOW_UID=$(id -u)
   ```

   Copy your GCP service account key (downloaded from Google Cloud Console):
   ```bash
   cp creds/gcp_example_account.example.json creds/gcp_service_account.json
   ```

3. **Start the application stack**
   Build and run all services with Docker Compose:
   ```bash
   docker compose up --build
   ```

4. **Access the local services**
   - **Airflow UI**: [http://localhost:18080](http://localhost:18080)
     *(username/password: `airflow` / `airflow`)
   - **Streamlit Dashboard**: [http://localhost:8700](http://localhost:8700)


### âš™ï¸ Environment Configuration

To keep credentials, configurations, and cloud settings organized, this project uses both a .env file and a YAML-based config file (`settings.yaml`).

This separation allows environment-specific settings to be reused across Airflow, PySpark, and custom scripts.

#### ğŸ” `.env` File (For Secrets and Runtime Variables)

This file contains sensitive information and environment runtime variables.
You can start by copying the example file:

```bash
cp .env.example .env
```

Then edit `.env` to fill in your own values:

```bash
GCP_PROJECT=your-project-id
BIGQUERY_DATASET=your_dataset
BIGQUERY_TABLE=your_table
BIGQUERY_SUMMARY_TABLE=your_summary_table
GCP_SERVICE_ACCOUNT_KEY_PATH=./creds/gcp_service_account.json
```

The `.env` file is loaded automatically by Airflow, the BigQuery upload script, and the Streamlit dashboard.

#### `settings.yaml` (Pipeline Parameters and Spark Config)

The `settings.yaml` file stores pipeline-specific parameters in a structured and readable format.

Example:
```bash
data_config:
  year: 2023
  months: [1, 2, 3]
  output_prefix: "yellow_tripdata"

spark_config:
  app_name: "YellowTaxiCleaning"
  master: "local[*]"

bigquery_config:
  project_id: ${BQ_PROJECT_ID}
  dataset: ${BQ_DATASET}
  table: ${BQ_TABLE}
  summary_table: ${BQ_SUMMARY_TABLE}
```

This file allows flexible control over:
   - The year and months of data to download
   - Spark cluster settings (e.g., `local[*]`)
   - Target BigQuery table names

Itâ€™s especially useful when triggering Airflow DAGs with dynamic inputs.

#### ğŸ³ Docker Image

This project includes a prebuilt Docker image hosted on Docker Hub.
To use it directly without building locally, follow the steps below.

ğŸ“¦ **Pull the images manually (optional)**
```bash
docker pull bravo634/nyc-taxi-airflow:latest
docker pull bravo634/nyc-taxi-app:latest
```
This is optional but recommended if you want to avoid building the image locally when running docker-compose up for the first time.

ğŸ› ï¸ Configure your environment
Make sure your .env or .env.example file contains:
```bash
DOCKER_IMAGE_PREFIX=bravo634
```
This will ensure all services (Airflow, Streamlit, etc.) use my public Docker images.

---

### â–¶ï¸ Local Run (Optional Quick Start Without Docker)

If you want to run parts of the pipeline locally without Docker (for quick testing or debugging), hereâ€™s how:

#### ğŸŒ€ Run Airflow DAG Components

You can run and debug individual DAG steps directly using:

```bash
python airflow_pipeline/scripts/main_debug.py
```

Make sure you have:
- Installed all dependencies from `airflow_pipeline/requirements.txt`
- Set up `.env` and `settings.yaml` in the `airflow_pipeline/config/ directory`

#### ğŸ“Š Run Streamlit Dashboard
To launch the Streamlit dashboard independently:

```bash
cd streamlit_app
streamlit run app.py
```

Make sure:
- Your `.env` file contains the correct BigQuery credentials
- The summary table already exists in BigQuery
- Youâ€™ve installed all dependencies from `streamlit_app/requirements.txt`

This is a lightweight way to test the core features without spinning up Docker.

### ğŸ“ Project Structure

The repository is organized to reflect a modular and production-like data engineering pipeline. Here's a quick overview of the main components:

```text
ğŸ“ Project Structure

.github/workflows/               # CI setup for GitHub Actions (optional)
â”œâ”€â”€ docker.yml                   # Auto-build Docker image and push on commit

.streamlit/                      # Streamlit UI configuration
â”œâ”€â”€ config.toml                  # Custom theme and layout settings

airflow_pipeline/                # Main directory for the Airflow ETL pipeline
â”œâ”€â”€ config/                      # Pipeline configuration files
â”‚   â”œâ”€â”€ airflow.cfg              # Airflow runtime settings
â”‚   â””â”€â”€ settings.yaml            # Pipeline parameters (e.g., year/month)
â”œâ”€â”€ dags/                        # DAG definitions for Airflow
â”‚   â””â”€â”€ nyc_taxi_etl_dag.py      # Main DAG for NYC Yellow Taxi data pipeline
â”œâ”€â”€ data/                        # Local data storage
â”‚   â”œâ”€â”€ processed/               # Cleaned CSV outputs
â”‚   â”œâ”€â”€ raw/                     # Original monthly Parquet files
â”‚   â””â”€â”€ tmp/                     # Intermediate temp files (optional)
â”œâ”€â”€ logs/                        # Logs generated during Airflow runs
â”œâ”€â”€ scripts/                     # Python helper modules used in DAG
â”‚   â”œâ”€â”€ config.py                # Loads .env and settings.yaml
â”‚   â”œâ”€â”€ get_bigquery_client.py   # Auth wrapper for BigQuery
â”‚   â”œâ”€â”€ load_nyc_yellow_taxi_data.py  # Downloads and saves monthly data
â”‚   â”œâ”€â”€ logger.py                # Custom logging utility
â”‚   â”œâ”€â”€ main_debug.py            # Local dev entry point for pipeline testing
â”‚   â”œâ”€â”€ sql_utils.py             # Run SQL queries (BigQuery)
â”‚   â”œâ”€â”€ transform_trip_data_spark.py  # Cleans data using PySpark
â”‚   â””â”€â”€ upload_to_big_query.py   # Uploads results to BigQuery
â”œâ”€â”€ sql/                         # SQL scripts used in the pipeline
â”‚   â”œâ”€â”€ bq/
â”‚   â”‚   â””â”€â”€ create_trip_summary_hourly.sql  # Creates hourly summary table in BigQuery
â”‚   â””â”€â”€ raw/
â”‚       â””â”€â”€ trip_summary_hourly.sql         # DuckDB-based SQL for local test

creds/                           # Local-only folder for GCP credentials
â””â”€â”€ gcp_service_account.example.json  # Example for service account key

notebooks/                       # Jupyter notebooks for exploration
â”œâ”€â”€ 01_explore_raw_data.ipynb    # Schema, volume, and column analysis
â”œâ”€â”€ 02_transform_trip_summary.ipynb  # Local test of data cleaning logic
â””â”€â”€ 03_visualize_summary.ipynb   # Plots and KPIs (merged from previous 04)

streamlit_app/                   # Interactive dashboard built with Streamlit
â”œâ”€â”€ external/                    # External resources (e.g. GeoJSON files)
â”‚   â””â”€â”€ taxi_zones.geojson       # NYC taxi zones for map rendering
â”œâ”€â”€ pages/                       # Modular pages for dashboard
â”‚   â”œâ”€â”€ Trend_Viewer.py          # Plots hourly trends over time
â”‚   â””â”€â”€ Zone_Heatmap.py          # Visualizes pickup/dropoff heatmap by zone
â”œâ”€â”€ utils/                       # Reusable functions for Streamlit app
â”‚   â”œâ”€â”€ config.py                # Loads config.yaml and env vars
â”‚   â”œâ”€â”€ get_bigquery_client.py   # BigQuery access for dashboard
â”‚   â”œâ”€â”€ logger.py                # Logger for diagnostics
â”‚   â””â”€â”€ sql_utils.py             # Shared SQL execution tools
â”œâ”€â”€ config.yaml                  # Dashboard config (e.g., date range, metrics)
â”œâ”€â”€ app.py                       # Main app entry point (use: `streamlit run app.py`)
â”œâ”€â”€ .dockerignore                # Prevents cache and temp files in image
â”œâ”€â”€ Dockerfile                   # Dockerfile for dashboard deployment
â””â”€â”€ requirements.txt             # Required Python packages for dashboard

.env.example                     # Template for environment variables
.gitignore                       # Git exclusions
docker-compose.yaml              # Main Docker orchestration file
docker-compose.override.yaml     # Local override (ports, mounts, etc.)
Makefile                         # CLI shortcuts (optional)
pyproject.toml                   # Python project metadata
README.md                        # This documentation
```

### ğŸ“‹ Future Improvements

This project was designed as a lightweight, end-to-end data engineering demo. However, several areas could be further enhanced:

- **Incremental Loads**  
  Currently, the pipeline performs full refreshes. Future versions could use timestamps or partitioning to only load new or updated records.

- **Data Validation**  
  Integrating a library like [Great Expectations](https://greatexpectations.io/) or using SQL tests would help validate raw and cleaned data for quality assurance.

- **Workflow Modularity**  
  The current Airflow DAG uses Python functions for each step. Refactoring with task groups or external task triggers would improve scalability.

- **Enhanced Visualizations**  
  The Streamlit dashboard can be expanded with filters, comparison tools, or animated charts (e.g., via `Plotly` or `Altair`).

- **Cloud Deployment**  
  Although the pipeline was tested locally, deploying Airflow via **Cloud Composer**, and hosting the dashboard on **Streamlit Cloud** or **GCP App Engine** would make it production-ready.

- **PySpark Optimization**  
  PySpark logic could be tuned to support multi-node processing, and caching could be introduced to avoid redundant computations.

---

### ğŸ§  Lessons Learned

This project started as a simple data upload task, but it gradually grew into a full-stack data pipeline. Over the course of building it, I learned to:

- Handle multiple moving parts in a data engineering stack (PySpark, Airflow, BigQuery, Streamlit)
- Use Docker and `.env` files to isolate environments
- Design modular code for orchestration and transformation
- Translate Jupyter-based logic into robust pipeline steps
- Troubleshoot real-world bugs like permission issues, timezone mismatches, and schema mismatches

The biggest takeaway: **engineering a data pipeline is not just about codeâ€”itâ€™s about orchestration, visibility, and maintainability.**

---

### ğŸ™Œ Acknowledgements

Special thanks to:

- NYC TLC for making the yellow taxi dataset publicly available  
- Google Cloudâ€™s generous free tier (BigQuery & Storage)  
- Airflow & Streamlit communities for excellent documentation  
- [DataTribe](https://github.com/datatribe-collective-labs/data-engineering-learnings/tree/main) for inspiring the capstone structure and workflow

---

### ğŸ“® Contact

Feel free to connect or share feedback:

- **GitHub**: [tuo-yang](https://github.com/TuoYang263)  
- **LinkedIn**: [tuo-yang-linkedin-profile](https://www.linkedin.com/in/tuo-yang-6b772b207/)
- **Email**: yangtuomailbox@gmail.com
